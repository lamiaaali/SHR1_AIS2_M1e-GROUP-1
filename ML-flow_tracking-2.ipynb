{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.pyfunc\n",
    "import tensorflow as tf\n",
    "from safetensors.tensorflow import load_file  \n",
    "\n",
    "input_shape = 1 \n",
    "\n",
    "class SafeTensorModel(mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "        self.model = self.build_model()  # Build the model when the instance is created\n",
    "\n",
    "    def build_model(self):\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.Input(shape=(self.input_shape,)),  \n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(1, activation='sigmoid')  \n",
    "        ])\n",
    "        return model\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        return self.model.predict(model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf1973b4d405460db8d5e0bc8cc43005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/19 09:38:18 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c51da580ccb4e09ab359c4918bf5683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import mlflow\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "from mlflow.models import ModelSignature\n",
    "from mlflow.types import Schema, ColSpec\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run() as run:\n",
    "\n",
    "    # Path to your model\n",
    "    model_directory = r\"D:\\Study\\DEPI-Gneratve AI\\Final_project\\chat2\\model\"\n",
    "\n",
    "    # Create a temporary directory to copy your model files (optional)\n",
    "    temp_directory = os.path.join(model_directory, \"temp_model_dir\")\n",
    "    os.makedirs(temp_directory, exist_ok=True)\n",
    "\n",
    "    # Copy the model files to the temporary directory\n",
    "    for file_name in os.listdir(model_directory):\n",
    "        full_file_name = os.path.join(model_directory, file_name)\n",
    "        if os.path.isfile(full_file_name):\n",
    "            shutil.copy(full_file_name, temp_directory)\n",
    "    \n",
    "    # Log the model architecture\n",
    "    mlflow.log_artifact(model_directory, artifact_path=\"safetensors_file\")\n",
    "\n",
    "    # Log the model\n",
    "    input_shape = 1  # Adjust according to your model's expected input shape\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"model\",\n",
    "        python_model=SafeTensorModel(input_shape=input_shape),\n",
    "        artifacts={\n",
    "            \"safetensors_file\": model_directory\n",
    "             }\n",
    "    )\n",
    "\n",
    "    # Log the model tokenizer\n",
    "    local_tokenizer_path = r\"D:\\Study\\DEPI-Gneratve AI\\Final_project\\chat2\\token\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(local_tokenizer_path)\n",
    "    \n",
    "    # Log all files in the tokenizer directory\n",
    "    for filename in os.listdir(local_tokenizer_path):\n",
    "        file_path = os.path.join(local_tokenizer_path, filename)\n",
    "        if os.path.isfile(file_path):  # Ensure it's a file\n",
    "            mlflow.log_artifact(file_path, artifact_path=\"tokenizer\")\n",
    "\n",
    "\n",
    "\n",
    "    # Load parameters from the JSON file\n",
    "    with open(\"D:\\Study\\DEPI-Gneratve AI\\Final_project\\chat2\\parameters-2.json\", 'r') as f:\n",
    "        params = json.load(f)\n",
    "\n",
    "    for key, value in params.items():\n",
    "         mlflow.log_param(key, value)\n",
    "\n",
    "    # Log metrics\n",
    "    for epoch in range(params['num_epochs']):\n",
    "        loss = 1.773  \n",
    "        mlflow.log_metric(\"loss\", loss, step=epoch)\n",
    "\n",
    "\n",
    "    # # # Example input data as a DataFrame\n",
    "    # input_example = \"i love cairo\"\n",
    "\n",
    "    # Define the input schema and output schema for the model signature\n",
    "    input_schema = Schema([\n",
    "        ColSpec(\"string\")\n",
    "    ])\n",
    "    output_schema = Schema([\n",
    "        ColSpec(\"string\")  # Adjust according to your output\n",
    "    ])\n",
    "\n",
    "    signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n",
    "\n",
    "\n",
    "    # Now log the model, passing the input_shape\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"model\",\n",
    "        python_model=SafeTensorModel(input_shape=input_shape),  # Pass input_shape here\n",
    "        artifacts={\n",
    "            \"safetensors_file\": os.path.join(temp_directory, \"model.safetensors\")  # Reference the safetensors file in the temp directory\n",
    "        },\n",
    "         code_paths=[temp_directory]  # This will log the entire directory    \n",
    "        ,\n",
    "        conda_env={\n",
    "            'channels': ['defaults'],\n",
    "            'dependencies': [\n",
    "                'python=3.9',  \n",
    "                'cloudpickle=3.1.0',\n",
    "                'pandas' \n",
    "            ],\n",
    "            'name': 'Gen_environment' \n",
    "        },\n",
    "        # input_example=input_example,  # Pass the input example as DataFrame\n",
    "        signature=signature  # Pass the model signature\n",
    "    )\n",
    "    # clean up the temporary directory after logging\n",
    "    shutil.rmtree(temp_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load and use the model later\n",
    "model_uri = f\"runs:/{run.info.run_id}/model\"\n",
    "loaded_model = mlflow.pyfunc.load_model(model_uri)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
