{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Installing Required Libraries\nThe following code installs the necessary libraries for our project. `datasets` is used for handling various datasets, and `transformers` provides access to pre-trained transformer models from Hugging Face. These libraries enable us to load and preprocess data efficiently and apply state-of-the-art machine learning models.\n","metadata":{}},{"cell_type":"code","source":"pip install datasets transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-19T02:36:27.427038Z","iopub.execute_input":"2024-10-19T02:36:27.427711Z","iopub.status.idle":"2024-10-19T02:36:40.394983Z","shell.execute_reply.started":"2024-10-19T02:36:27.427646Z","shell.execute_reply":"2024-10-19T02:36:40.393761Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: huggingface-hub>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.25.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Loading the Dataset\nWe load the dataset using the `load_dataset` function from the `datasets` library.","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\ndatasets = load_dataset('wikitext', 'wikitext-2-raw-v1')","metadata":{"execution":{"iopub.status.busy":"2024-10-19T02:36:56.776005Z","iopub.execute_input":"2024-10-19T02:36:56.776851Z","iopub.status.idle":"2024-10-19T02:37:03.761013Z","shell.execute_reply.started":"2024-10-19T02:36:56.776808Z","shell.execute_reply":"2024-10-19T02:37:03.760005Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0aa94b632d5248b695d8e0f3d05034d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/733k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cc2d2b004814232af548ad510032ad5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/6.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a621b258834c47f59a96fec0c4ad46cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/657k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88d317505633404d94a8b1e57eef21bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f51d8024daf4e8dba6598488adc1aa4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db8ee7b7d3f54a11b0dcf1585d5daca2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dadce6ea85aa4f8bbed0125bcaf4a09b"}},"metadata":{}}]},{"cell_type":"code","source":"datasets[\"train\"][10]","metadata":{"execution":{"iopub.status.busy":"2024-10-19T02:37:08.937947Z","iopub.execute_input":"2024-10-19T02:37:08.938517Z","iopub.status.idle":"2024-10-19T02:37:08.948229Z","shell.execute_reply.started":"2024-10-19T02:37:08.938478Z","shell.execute_reply":"2024-10-19T02:37:08.947317Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"{'text': ' The game \\'s battle system , the BliTZ system , is carried over directly from Valkyira Chronicles . During missions , players select each unit using a top @-@ down perspective of the battlefield map : once a character is selected , the player moves the character around the battlefield in third @-@ person . A character can only act once per @-@ turn , but characters can be granted multiple turns at the expense of other characters \\' turns . Each character has a field and distance of movement limited by their Action Gauge . Up to nine characters can be assigned to a single mission . During gameplay , characters will call out if something happens to them , such as their health points ( HP ) getting low or being knocked out by enemy attacks . Each character has specific \" Potentials \" , skills unique to each character . They are divided into \" Personal Potential \" , which are innate skills that remain unaltered unless otherwise dictated by the story and can either help or impede a character , and \" Battle Potentials \" , which are grown throughout the game and always grant boons to a character . To learn Battle Potentials , each character has a unique \" Masters Table \" , a grid @-@ based skill table that can be used to acquire and link different skills . Characters also have Special Abilities that grant them temporary boosts on the battlefield : Kurt can activate \" Direct Command \" and move around the battlefield without depleting his Action Point gauge , the character Reila can shift into her \" Valkyria Form \" and become invincible , while Imca can target multiple enemy units with her heavy weapon . \\n'}"},"metadata":{}}]},{"cell_type":"markdown","source":"## Function to Display Random Dataset Elements\nThis function `show_random_elements` helps us visualize random samples from the dataset. It takes a dataset and a specified number of examples, then displays them in a tabular format. The `ClassLabel` feature allows us to convert integer labels into human-readable names, improving readability. This is useful for quickly inspecting the data.\n","metadata":{}},{"cell_type":"code","source":"from datasets import ClassLabel\nimport random\nimport pandas as pd\nfrom IPython.display import display, HTML\n\ndef show_random_elements(dataset, num_examples=10):\n    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n    picks = []\n    for _ in range(num_examples):\n        pick = random.randint(0, len(dataset)-1)\n        while pick in picks:\n            pick = random.randint(0, len(dataset)-1)\n        picks.append(pick)\n    \n    df = pd.DataFrame(dataset[picks])\n    for column, typ in dataset.features.items():\n        if isinstance(typ, ClassLabel):\n            df[column] = df[column].transform(lambda i: typ.names[i])\n    display(HTML(df.to_html()))","metadata":{"execution":{"iopub.status.busy":"2024-10-19T02:37:09.745474Z","iopub.execute_input":"2024-10-19T02:37:09.745869Z","iopub.status.idle":"2024-10-19T02:37:09.754361Z","shell.execute_reply.started":"2024-10-19T02:37:09.745830Z","shell.execute_reply":"2024-10-19T02:37:09.753372Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Displaying Random Samples from the Dataset\nHere we use the previously defined `show_random_elements` function to view a random selection of samples from the training dataset. This helps us verify that the data was loaded correctly and provides insights into its structure before further processing.\n","metadata":{}},{"cell_type":"code","source":"show_random_elements(datasets[\"train\"])","metadata":{"execution":{"iopub.status.busy":"2024-10-19T02:37:16.551736Z","iopub.execute_input":"2024-10-19T02:37:16.552108Z","iopub.status.idle":"2024-10-19T02:37:16.568519Z","shell.execute_reply.started":"2024-10-19T02:37:16.552070Z","shell.execute_reply":"2024-10-19T02:37:16.567637Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td></td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Kieswetter 's performances in the 2009 season led to his inclusion in the England Performance Programme squad in November and December of that year , and he was part of the England Lions squad which toured the United Arab Emirates in early 2010 , along with Somerset team @-@ mate Peter Trego . His full England debut came shortly after in Bangladesh . Additionally , two of Somerset 's young players , Jos Buttler and Calum Haggett played for England Under @-@ 19s during the English winter . \\n</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td></td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>= Mothers of the Disappeared = \\n</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Florida panther ( P. c. coryi ) \\n</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>John Balfour was the British ambassador in Argentina during the Perón regime , and describes Evita 's popularity : \\n</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td></td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td></td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td></td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td></td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}]},{"cell_type":"code","source":"model_checkpoint = \"openai-community/gpt2\"\ntokenizer_checkpoint = \"openai-community/gpt2\"","metadata":{"execution":{"iopub.status.busy":"2024-10-19T02:37:17.554443Z","iopub.execute_input":"2024-10-19T02:37:17.555297Z","iopub.status.idle":"2024-10-19T02:37:17.559272Z","shell.execute_reply.started":"2024-10-19T02:37:17.555260Z","shell.execute_reply":"2024-10-19T02:37:17.558246Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n    \ntokenizer = AutoTokenizer.from_pretrained(tokenizer_checkpoint)","metadata":{"execution":{"iopub.status.busy":"2024-10-19T02:37:24.649995Z","iopub.execute_input":"2024-10-19T02:37:24.650419Z","iopub.status.idle":"2024-10-19T02:37:30.777085Z","shell.execute_reply.started":"2024-10-19T02:37:24.650376Z","shell.execute_reply":"2024-10-19T02:37:30.776165Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4df7baad093478095e94bd295d263ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49df2c66d12f42ffb11471bdbc225ef9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"179925af0f094fb18cae0226685c4108"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24af548b0d5a4fb49a8dd1b252531a48"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f92b5b4e05404529b83dc7ef8d37cfd4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df66c1679e564af2bdc8f18120e7b0e2"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Tokenization Function\nThis code defines the `tokenize_function`, which tokenizes each text example in the dataset. Tokenization is a critical preprocessing step in NLP as it breaks down the input text into tokens (words or subwords), which are the basic units processed by transformer models. We will use this function to prepare the dataset for model training.\n","metadata":{}},{"cell_type":"code","source":"def tokenize_function(examples):\n    return tokenizer(examples[\"text\"])","metadata":{"execution":{"iopub.status.busy":"2024-10-19T02:37:30.778540Z","iopub.execute_input":"2024-10-19T02:37:30.779018Z","iopub.status.idle":"2024-10-19T02:37:30.785877Z","shell.execute_reply.started":"2024-10-19T02:37:30.778982Z","shell.execute_reply":"2024-10-19T02:37:30.784907Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Applying Tokenization to the Dataset\nThis cell applies the `tokenize_function` we defined earlier to the entire dataset. The `map` method allows us to transform each example in the dataset by applying our tokenization function. Tokenizing the dataset is essential for preparing the text data so that it can be processed by the model.\n","metadata":{}},{"cell_type":"code","source":"tokenized_datasets = datasets.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"])\n","metadata":{"execution":{"iopub.status.busy":"2024-10-19T02:37:36.482842Z","iopub.execute_input":"2024-10-19T02:37:36.483213Z","iopub.status.idle":"2024-10-19T02:37:42.322052Z","shell.execute_reply.started":"2024-10-19T02:37:36.483176Z","shell.execute_reply":"2024-10-19T02:37:42.321023Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"551350887980410bac800fe7ef85cd9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/36718 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32b0b0c91c3d49879ab3f4c6df6f925f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0081b97cd4749d1bf7988d0c9eb3da3"}},"metadata":{}}]},{"cell_type":"code","source":"tokenized_datasets[\"train\"][1]\n","metadata":{"execution":{"iopub.status.busy":"2024-10-19T02:38:17.123094Z","iopub.execute_input":"2024-10-19T02:38:17.124038Z","iopub.status.idle":"2024-10-19T02:38:17.133619Z","shell.execute_reply.started":"2024-10-19T02:38:17.123989Z","shell.execute_reply":"2024-10-19T02:38:17.132538Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"{'input_ids': [796, 569, 18354, 7496, 17740, 6711, 796, 220, 198],\n 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"},"metadata":{}}]},{"cell_type":"code","source":"# block_size = tokenizer.model_max_length\nblock_size = 128","metadata":{"execution":{"iopub.status.busy":"2024-10-19T02:38:20.242470Z","iopub.execute_input":"2024-10-19T02:38:20.243173Z","iopub.status.idle":"2024-10-19T02:38:20.247767Z","shell.execute_reply.started":"2024-10-19T02:38:20.243113Z","shell.execute_reply":"2024-10-19T02:38:20.246694Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Function: `group_texts`\n\nThe `group_texts` function processes a dictionary of text examples, transforming them into fixed-size blocks suitable for model training or evaluation. Below is a comprehensive overview of its functionality:\n","metadata":{}},{"cell_type":"code","source":"def group_texts(examples):\n    # Concatenate all texts.\n    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n    total_length = len(concatenated_examples[list(examples.keys())[0]])\n    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n        # customize this part to your needs.\n    total_length = (total_length // block_size) * block_size\n    # Split by chunks of max_len.\n    result = {\n        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n        for k, t in concatenated_examples.items()\n    }\n    result[\"labels\"] = result[\"input_ids\"].copy()\n    return result","metadata":{"execution":{"iopub.status.busy":"2024-10-19T02:38:22.197175Z","iopub.execute_input":"2024-10-19T02:38:22.197868Z","iopub.status.idle":"2024-10-19T02:38:22.204856Z","shell.execute_reply.started":"2024-10-19T02:38:22.197826Z","shell.execute_reply":"2024-10-19T02:38:22.203762Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"lm_datasets = tokenized_datasets.map(\n    group_texts,\n    batched=True,\n    batch_size=1000,\n    num_proc=4,\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-19T02:38:26.309651Z","iopub.execute_input":"2024-10-19T02:38:26.310748Z","iopub.status.idle":"2024-10-19T02:38:34.189074Z","shell.execute_reply.started":"2024-10-19T02:38:26.310697Z","shell.execute_reply":"2024-10-19T02:38:34.187989Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dea3a43053794ed7b02115a1f8a5c840"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/36718 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e724eb3e136345faa6f3ecca97e563e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29f8affc46794da9b308ca067d416a13"}},"metadata":{}}]},{"cell_type":"code","source":"tokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"])","metadata":{"execution":{"iopub.status.busy":"2024-10-19T02:38:34.191242Z","iopub.execute_input":"2024-10-19T02:38:34.191580Z","iopub.status.idle":"2024-10-19T02:38:46.443755Z","shell.execute_reply.started":"2024-10-19T02:38:34.191544Z","shell.execute_reply":"2024-10-19T02:38:46.442612Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"' game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . \\n The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Oz'"},"metadata":{}}]},{"cell_type":"markdown","source":"## Loading the Model and Training Arguments","metadata":{}},{"cell_type":"code","source":"from transformers import AutoConfig, AutoModelForCausalLM\n\nconfig = AutoConfig.from_pretrained(model_checkpoint)\nmodel = AutoModelForCausalLM.from_config(config)","metadata":{"execution":{"iopub.status.busy":"2024-10-19T02:38:46.445865Z","iopub.execute_input":"2024-10-19T02:38:46.447182Z","iopub.status.idle":"2024-10-19T02:38:50.432076Z","shell.execute_reply.started":"2024-10-19T02:38:46.447131Z","shell.execute_reply":"2024-10-19T02:38:50.430957Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\n","metadata":{"execution":{"iopub.status.busy":"2024-10-19T02:38:57.783033Z","iopub.execute_input":"2024-10-19T02:38:57.784053Z","iopub.status.idle":"2024-10-19T02:38:59.723603Z","shell.execute_reply.started":"2024-10-19T02:38:57.784004Z","shell.execute_reply":"2024-10-19T02:38:59.722804Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## Defining Training Arguments\nThe `TrainingArguments` object defines the configuration for training the model. These arguments include parameters such as the learning rate, weight decay , evaluation strategy, and where to store model checkpoints. By adjusting these parameters, we can fine-tune the training process to fit our hardware and requirements.\n","metadata":{}},{"cell_type":"code","source":"training_args = TrainingArguments(\n    f\"{model_checkpoint}-wikitext2\",\n    eval_strategy = \"epoch\",\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    push_to_hub=True\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-19T02:39:01.841763Z","iopub.execute_input":"2024-10-19T02:39:01.842479Z","iopub.status.idle":"2024-10-19T02:39:01.958539Z","shell.execute_reply.started":"2024-10-19T02:39:01.842438Z","shell.execute_reply":"2024-10-19T02:39:01.957557Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-10-19T02:39:05.046290Z","iopub.execute_input":"2024-10-19T02:39:05.046777Z","iopub.status.idle":"2024-10-19T02:39:05.073184Z","shell.execute_reply.started":"2024-10-19T02:39:05.046710Z","shell.execute_reply":"2024-10-19T02:39:05.072257Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4eb66ee75c14a2db0f8c3a30f01c646"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Initializing the Trainer\n\nThe following code snippet demonstrates how to create a `Trainer` instance for training a language model using the Hugging Face Transformers library. The `Trainer` class simplifies the training and evaluation process by encapsulating common functionalities.","metadata":{}},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=lm_datasets[\"train\"],\n    eval_dataset=lm_datasets[\"validation\"],\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-19T02:39:25.005282Z","iopub.execute_input":"2024-10-19T02:39:25.006101Z","iopub.status.idle":"2024-10-19T02:39:26.220941Z","shell.execute_reply.started":"2024-10-19T02:39:25.006057Z","shell.execute_reply":"2024-10-19T02:39:26.219714Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-10-19T02:39:30.378095Z","iopub.execute_input":"2024-10-19T02:39:30.378531Z","iopub.status.idle":"2024-10-19T03:05:52.311733Z","shell.execute_reply.started":"2024-10-19T02:39:30.378469Z","shell.execute_reply":"2024-10-19T03:05:52.310946Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112763166666657, max=1.0…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a5c345db26a4a50bde8688659e1c4ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241019_023951-omjwgezr</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mohamedtalaatrrr-fci-zu-edu/huggingface/runs/omjwgezr' target=\"_blank\">openai-community/gpt2-wikitext2</a></strong> to <a href='https://wandb.ai/mohamedtalaatrrr-fci-zu-edu/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mohamedtalaatrrr-fci-zu-edu/huggingface' target=\"_blank\">https://wandb.ai/mohamedtalaatrrr-fci-zu-edu/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mohamedtalaatrrr-fci-zu-edu/huggingface/runs/omjwgezr' target=\"_blank\">https://wandb.ai/mohamedtalaatrrr-fci-zu-edu/huggingface/runs/omjwgezr</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3501' max='3501' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3501/3501 25:56, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>6.632900</td>\n      <td>6.468777</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>6.255800</td>\n      <td>6.238887</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>6.055200</td>\n      <td>6.169648</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=3501, training_loss=6.428747082873434, metrics={'train_runtime': 1580.4784, 'train_samples_per_second': 35.431, 'train_steps_per_second': 2.215, 'total_flos': 3657957801984000.0, 'train_loss': 6.428747082873434, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"## Evaluating the Model and Calculating Perplexity\n\nThis code snippet demonstrates how to evaluate a trained language model using the `Trainer` class from the Hugging Face Transformers library. It also calculates the perplexity of the model based on the evaluation loss.","metadata":{}},{"cell_type":"code","source":"import math\neval_results = trainer.evaluate()\nprint(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-19T03:05:57.973810Z","iopub.execute_input":"2024-10-19T03:05:57.974741Z","iopub.status.idle":"2024-10-19T03:06:16.573862Z","shell.execute_reply.started":"2024-10-19T03:05:57.974682Z","shell.execute_reply":"2024-10-19T03:06:16.572927Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='121' max='121' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [121/121 00:18]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Perplexity: 478.02\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.push_to_hub(\"gpt2-model\")","metadata":{"execution":{"iopub.status.busy":"2024-10-19T03:08:01.692510Z","iopub.execute_input":"2024-10-19T03:08:01.693680Z","iopub.status.idle":"2024-10-19T03:08:05.247847Z","shell.execute_reply.started":"2024-10-19T03:08:01.693608Z","shell.execute_reply":"2024-10-19T03:08:05.246834Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"No files have been modified since last commit. Skipping to prevent empty commit.\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/MohamedTalaat91/gpt2-wikitext2/commit/c39ebbbc171839c218fc35248d9ed3939534cdec', commit_message='gpt2-model', commit_description='', oid='c39ebbbc171839c218fc35248d9ed3939534cdec', pr_url=None, repo_url=RepoUrl('https://huggingface.co/MohamedTalaat91/gpt2-wikitext2', endpoint='https://huggingface.co', repo_type='model', repo_id='MohamedTalaat91/gpt2-wikitext2'), pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"trainer.save_model(\"gpt2-wikitext2\")","metadata":{"execution":{"iopub.status.busy":"2024-10-19T03:29:18.831580Z","iopub.execute_input":"2024-10-19T03:29:18.831982Z","iopub.status.idle":"2024-10-19T03:29:23.375114Z","shell.execute_reply.started":"2024-10-19T03:29:18.831945Z","shell.execute_reply":"2024-10-19T03:29:23.374164Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stderr","text":"No files have been modified since last commit. Skipping to prevent empty commit.\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer.save_pretrained(\"gpt2-wikitext2\")","metadata":{"execution":{"iopub.status.busy":"2024-10-19T03:27:51.952151Z","iopub.execute_input":"2024-10-19T03:27:51.952904Z","iopub.status.idle":"2024-10-19T03:27:52.022623Z","shell.execute_reply.started":"2024-10-19T03:27:51.952860Z","shell.execute_reply":"2024-10-19T03:27:52.021617Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"('gpt2-wikitext2/tokenizer_config.json',\n 'gpt2-wikitext2/special_tokens_map.json',\n 'gpt2-wikitext2/vocab.json',\n 'gpt2-wikitext2/merges.txt',\n 'gpt2-wikitext2/added_tokens.json',\n 'gpt2-wikitext2/tokenizer.json')"},"metadata":{}}]},{"cell_type":"markdown","source":"## Loading a Retrained Model and Tokenizer\n\nThis code snippet demonstrates how to load a pre-trained language model and its associated tokenizer using the Hugging Face Transformers library. It prepares an input text for further processing.\n","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Load the retrained model\nmodel_path = '/kaggle/working/gpt2-wikitext2'\nmodel = AutoModelForCausalLM.from_pretrained(model_path)\n\n# Load the tokenizer associated with the model\ntokenizer = AutoTokenizer.from_pretrained(model_path)","metadata":{"execution":{"iopub.status.busy":"2024-10-19T03:29:28.863846Z","iopub.execute_input":"2024-10-19T03:29:28.864935Z","iopub.status.idle":"2024-10-19T03:29:29.184381Z","shell.execute_reply.started":"2024-10-19T03:29:28.864880Z","shell.execute_reply":"2024-10-19T03:29:29.183533Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"input_text = \"year study of the distribution\"\ninputs = tokenizer(input_text, return_tensors=\"pt\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-19T03:29:32.492564Z","iopub.execute_input":"2024-10-19T03:29:32.493251Z","iopub.status.idle":"2024-10-19T03:29:32.498757Z","shell.execute_reply.started":"2024-10-19T03:29:32.493210Z","shell.execute_reply":"2024-10-19T03:29:32.497728Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"## Generating Text with the Model\n\nThis code snippet demonstrates how to generate text based on an input using a pre-trained language model from the Hugging Face Transformers library. The model generates a continuation of the input text with specified parameters to control the generation process.\n","metadata":{}},{"cell_type":"code","source":"# Generate text based on the input\ngenerated_ids = model.generate(\n    inputs['input_ids'], \n    max_length=100,  # Adjust the max length as needed\n    num_return_sequences=1,  # Number of texts to generate\n    do_sample=True,  # Enable sampling (as opposed to greedy search)\n    top_k=50,  # Top-k sampling to introduce diversity\n    temperature=0.9  # Controls randomness in sampling\n)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-19T03:33:40.250433Z","iopub.execute_input":"2024-10-19T03:33:40.251103Z","iopub.status.idle":"2024-10-19T03:33:43.503908Z","shell.execute_reply.started":"2024-10-19T03:33:40.251059Z","shell.execute_reply":"2024-10-19T03:33:43.502813Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Decode the generated text to human-readable format\ngenerated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n\nprint(generated_text)","metadata":{"execution":{"iopub.status.busy":"2024-10-19T03:33:43.506724Z","iopub.execute_input":"2024-10-19T03:33:43.507032Z","iopub.status.idle":"2024-10-19T03:33:43.515574Z","shell.execute_reply.started":"2024-10-19T03:33:43.507000Z","shell.execute_reply":"2024-10-19T03:33:43.513701Z"},"trusted":true},"execution_count":74,"outputs":[{"name":"stdout","text":"year study of the distribution of the time of the only two part as the first , this time the American government which were still have been to have been a great part of the two months . \n = = = = = \n \n The same new year was named by the season of the United Kingdom of the storm and their own . The first , a single was released on the state as a new main series of the first time . The town was not be of a number of the island\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}